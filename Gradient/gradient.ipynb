{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a bit of testing of the back! function in Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 2.0]\n",
      "[10.0, 10.0]\n",
      "[12.0, 12.0]\n",
      "[2.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "using Flux, Flux.Tracker\n",
    "\n",
    "W = param([1., 1.])\n",
    "predict(x) = W * x        # prediction function\n",
    "\n",
    "loss(x, y) = sum((predict(x) .- y) .^ 2) # loss function\n",
    "\n",
    "x = 1\n",
    "y = 0\n",
    "\n",
    "l = loss(x, y)\n",
    "back!(l)\n",
    "println(W.grad) # gradient should be [2, 2]\n",
    "\n",
    "x = 2\n",
    "l = loss(x, y)\n",
    "back!(l)        # gradient should be [8, 8]\n",
    "println(W.grad) # it shows [10, 10] so back! sums gradients together\n",
    "\n",
    "x = 1\n",
    "l = loss(x, y)\n",
    "back!(l)        # gradient should be [2, 2]\n",
    "println(W.grad) # it shows [12, 12] so back! definitely does it\n",
    "\n",
    "W = param([1., 1.])\n",
    "l = loss(x, y)\n",
    "back!(l)\n",
    "println(W.grad) # new weights reset it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0, 4.0]\n",
      "[6.0, 6.0]\n"
     ]
    }
   ],
   "source": [
    "W = param([1., 1.])\n",
    "\n",
    "mutable struct Lparam\n",
    "    p::Float64\n",
    "end\n",
    "\n",
    "lossParam = Lparam(2.)\n",
    "\n",
    "loss(x, y) = lossParam.p * sum((predict(x) .- y) .^ 2) # create parametrical loss\n",
    "\n",
    "l = loss(x, y)\n",
    "back!(l)\n",
    "println(W.grad)  # gradient is [4, 4]\n",
    "\n",
    "lossParam.p = 1. # adjust the p\n",
    "\n",
    "l = loss(x, y)\n",
    "back!(l)\n",
    "println(W.grad) # gradient is [2, 2] and it shows [6, 6] which is correct in sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0]\n",
      "[3.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "W = param([1., 1.])\n",
    "lossParam = Lparam(1.)\n",
    "loss(x, y) = sum((predict(x) .- y) .^ lossParam.p) # create parametrical loss\n",
    "\n",
    "l = loss(x, y)\n",
    "back!(l)\n",
    "println(W.grad) # gradient is [1, 1]\n",
    "\n",
    "lossParam.p = 2. # adjust the p\n",
    "\n",
    "l = loss(x, y)\n",
    "back!(l)\n",
    "println(W.grad) # gradient is [2, 2] and it shows [3, 3] which is correct in sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
