module KNNmem
#= This module implements a memory according to https://arxiv.org/abs/1703.03129
    that can be used as the last layer in a NN in the Flux framework.
=#

using Flux

export KNNmemory, query, trainQuery!

mutable struct KNNmemory
    M::Array{Float64, 2} # keys in the memory
    V::Array{Int64, 1} # values in the memory
    A::Array{Int64, 1} # age of a given key-value pair
    k::Integer # number of neighbors used in kNN
    α::Real # parameter setting the required distance between the nearest positive and negative sample in the kNN

    function KNNmemory(memorySize::Integer, keySize::Integer, k::Integer, labelCount::Integer, α::Float64 = 0.1)
        M = rand(Float64, memorySize, keySize)
        V = rand(0:(labelCount - 1), memorySize)
        A = zeros(Int64, memorySize)

        for i = 1:memorySize
            M[i,:] = normalize(M[i,:])
        end

        new(M, V, A, k > memorySize ? memorySize : k, α)
    end
end

# Partitions the list l into chunks of the length n
partition(list, n) = [list[i:min(i + n - 1,length(list))] for i in 1:n:length(list)]

# query just returns the nearest neighbour's value for a given key q
function query(memory::KNNmemory, q::AbstractArray{Float64, N} where N)
    similarity = memory.M * normalizeQuery(Flux.Tracker.data(q))
    values = memory.V[Flux.argmax(similarity)]
    probabilities = maximum(softmax(similarity), 1)
    return values, probabilities
end

# For given set of k nearest neighbours, find the closest that has the same label and a different label
function findNearestPositiveAndNegative(memory::KNNmemory, kLargestIDs::Array{Int64, 1}, v::Integer)
    nearestPositiveID = nothing
    nearestNegativeID = nothing

    # typically this should not result into too many iterations
    for i in 1:memory.k
        if nearestPositiveID == nothing && memory.V[kLargestIDs[i]] == v
            nearestPositiveID = kLargestIDs[i]
        end
        if nearestNegativeID == nothing && memory.V[kLargestIDs[i]] != v
            nearestNegativeID = kLargestIDs[i]
        end
        if nearestPositiveID != nothing && nearestNegativeID != nothing
            break
        end
    end

    #= We assume that there exists such i that memory.V[i] == v
        and also such j that memory.V[j] != v

        We also assume that this won't happen very often, otherwise,
        we would need to randomize this selection (possible TODO) =#

    if nearestPositiveID == nothing
        nearestPositiveID = indmax(memory.V .== v)
    end
    if nearestNegativeID == nothing
        nearestNegativeID = indmax(memory.V .!= v)
    end

    return nearestPositiveID, nearestNegativeID
end

# Loss generated by the memory based on the lookup of a key-value pair - exactly as in the paper
memoryLoss(memory::KNNmemory, q::AbstractArray{Float64, 1}, nearestPosAndNegIDs::Tuple) = memoryLoss(memory, q, nearestPosAndNegIDs...)

# The inbuild function normalize() does not work with tracked vectors, this one does
trackedNormalize(x) = x / norm(x)

# Normalizes all queries in the matrix separately by converting it into an array of arrays
normalizeQuery(q) = hcat((trackedNormalize.([q[:, i] for i in 1:size(q, 2)]))...)

function memoryLoss(memory::KNNmemory, q::AbstractArray{Float64, 1}, nearestPositiveID::Integer, nearestNegativeID::Integer)
    loss = max(dot(trackedNormalize(q), memory.M[nearestNegativeID, :]) - dot(trackedNormalize(q), memory.M[nearestPositiveID, :]) + memory.α, 0)
end

# function that computes the appropriate update of the memory after a key-value pairpush!(LOAD_PATH, "/home/jan/dev/OSL/KNNmemory", "/home/jan/dev/anomaly detection/anomaly_detection/src") was lookedup in it
function memoryUpdate!(memory::KNNmemory, q::AbstractArray{Float64, 1}, v::Integer, nearestNeighbourID::Integer)
    # If the memory return the correct value for the given key, update the centroid
    if memory.V[nearestNeighbourID] == v
        memory.M[nearestNeighbourID, :] = normalize(q + memory.M[nearestNeighbourID, :])
        memory.A[nearestNeighbourID] = 0

    # If the memory did not return the correct value for the given key, store the key-value pair instead of the oldest element
    else
        oldestElementID = indmax(memory.A + rand(1:5))
        memory.M[oldestElementID, :] = q
        memory.V[oldestElementID] = v
        memory.A[oldestElementID] = 0
    end
end

# Update the age of all items
function increaseMemoryAge(memory::KNNmemory)
    memory.A = memory.A + 1;
end

trainQuery!(memory::KNNmemory, q::AbstractArray{Float64, N} where N, v::Int64) = trainQuery!(memory, q, [v])
# Batch version of the trainQuery
function trainQuery!(memory::KNNmemory, q::AbstractArray{Float64, N} where N, v::Array{Int64, 1})
    # Find k nearest neighbours and compute losses
    batchSize = size(q, 2)
    normalizedQuery = normalizeQuery(Flux.Tracker.data(q))
    similarity = memory.M * normalizedQuery # computes all similarities of all qs and all keys in the memory at once
    loss::Flux.Tracker.TrackedReal{Float64} = 0. # loss must be tracked; otherwise flux cannot use it
    nearestNeighbourIDs = zeros(Integer, batchSize)

    for i in 1:batchSize
        kLargestIDs = selectperm(similarity[:, i], 1:memory.k, rev = true)
        nearestNeighbourIDs[i] = kLargestIDs[1];
        loss = loss + memoryLoss(memory, q[:, i], findNearestPositiveAndNegative(memory, kLargestIDs, v[i]))
    end

    # Memory update - cannot be done above because we have to compute all losses before changing the memory
    for i in 1:batchSize
        memoryUpdate!(memory, normalizedQuery[:, i], v[i], nearestNeighbourIDs[i])
    end
    increaseMemoryAge(memory)

    return loss / batchSize
end
end
