module KNNmem
#= This module implements a memory according to https://arxiv.org/abs/1703.03129
    that can be used as the last layer in a NN in the Flux framework.
=#

using Flux

export KNNmemory, query, trainQuery!, augmentModelWithMemory

mutable struct KNNmemory{T <: Real}
    M::Matrix{T} # keys in the memory
    V::Vector{<:Integer} # values in the memory (labels)
    A::Vector{<:Integer} # age of a given key-value pair
    k::Integer # number of neighbors used in kNN
    α::Real # parameter setting the required distance between the nearest positive and negative sample in the kNN

    function KNNmemory{T}(memorySize::Integer, keySize::Integer, k::Integer, labelCount::Integer, α::Real = 0.1) where T
        M = rand(T, memorySize, keySize)
        V = rand(0:(labelCount - 1), memorySize)
        A = zeros(Int, memorySize)

        for i = 1:memorySize
            M[i,:] = normalize(M[i,:])
        end

        new(M, V, A, k > memorySize ? memorySize : k, α)
    end
end

# For given set of k nearest neighbours, find the closest two that have the same label and a different label respectively
function findNearestPositiveAndNegative(memory::KNNmemory, kLargestIDs::Vector{<:Integer}, v::Integer)
    nearestPositiveID = nothing
    nearestNegativeID = nothing

    # typically this should not result into too many iterations
    for i in 1:memory.k
        if nearestPositiveID == nothing && memory.V[kLargestIDs[i]] == v
            nearestPositiveID = kLargestIDs[i]
        end
        if nearestNegativeID == nothing && memory.V[kLargestIDs[i]] != v
            nearestNegativeID = kLargestIDs[i]
        end
        if nearestPositiveID != nothing && nearestNegativeID != nothing
            break
        end
    end

    #= We assume that there exists such i that memory.V[i] == v
        and also such j that memory.V[j] != v
list
        We also assume that this won't happen very often, otherwise,
        we would need to randomize this selection (possible TODO) =#

    if nearestPositiveID == nothing
        nearestPositiveID = indmax(memory.V .== v)
    end
    if nearestNegativeID == nothing
        nearestNegativeID = indmax(memory.V .!= v)
    end

    return nearestPositiveID, nearestNegativeID
end

# Loss generated by the memory based on the lookup of a key-value pair - exactly as in the paper
memoryLoss(memory::KNNmemory{T}, q::AbstractArray{T, 1}, nearestPosAndNegIDs::Tuple) where {T} = memoryLoss(memory, q, nearestPosAndNegIDs...)

function memoryLoss(memory::KNNmemory{T}, q::AbstractArray{T, 1}, nearestPositiveID::Integer, nearestNegativeID::Integer) where {T}
    loss = max(dot(normalize(q), memory.M[nearestNegativeID, :]) - dot(normalize(q), memory.M[nearestPositiveID, :]) + memory.α, 0)
end

# The inbuild function normalize() does not work with tracked vectors, this one does
Base.normalize(v::Flux.Tracker.TrackedArray{T}) where {T <: Real} = v ./ (sqrt(sum(v .^ 2) + eps(T)))

# Normalizes all queries in the matrix separately by converting it into an array of arrays
normalizeQuery(q) = hcat((normalize.([q[:, i] for i in 1:size(q, 2)]))...)

# function that computes the appropriate update of the memory after a key-value pairpush!(LOAD_PATH, "/home/jan/dev/OSL/KNNmemory", "/home/jan/dev/anomaly detection/anomaly_detection/src") was lookedup in it
function memoryUpdate!(memory::KNNmemory{T}, q::Vector{T}, v::Integer, nearestNeighbourID::Integer) where {T}
    # If the memory return the correct value for the given key, update the centroid
    if memory.V[nearestNeighbourID] == v
        memory.M[nearestNeighbourID, :] = normalize(q + memory.M[nearestNeighbourID, :])
        memory.A[nearestNeighbourID] = 0

    # If the memory did not return the correct value for the given key, store the key-value pair instead of the oldest element
    else
        oldestElementID = indmax(memory.A + rand(1:5))
        memory.M[oldestElementID, :] = q
        memory.V[oldestElementID] = v
        memory.A[oldestElementID] = 0
    end
end

# Update the age of all items
function increaseMemoryAge(memory::KNNmemory)
    memory.A += 1;
end

# query just returns the nearest neighbour's value for a given key q but does not modify the memory itself
function query(memory::KNNmemory{T}, q::AbstractArray{T, N} where N) where {T}
    similarity = memory.M * normalizeQuery(Flux.Tracker.data(q))
    values = memory.V[Flux.argmax(similarity)]
    probabilities = maximum(softmax(similarity), 1)
    return values, probabilities
end

# Query to the memory that does update its content and returns a loss
trainQuery!(memory::KNNmemory{T}, q::AbstractArray{T, N} where N, v::Integer) where {T} = trainQuery!(memory, q, [v])

function trainQuery!(memory::KNNmemory{T}, q::AbstractArray{T, N} where N, v::Vector{<:Integer}) where {T}
    # Find k nearest neighbours and compute losses
    batchSize = size(q, 2)
    normalizedQuery = normalizeQuery(Flux.Tracker.data(q))
    similarity = memory.M * normalizedQuery # computes all similarities of all qs and all keys in the memory at once
    loss::Flux.Tracker.TrackedReal{Float32} = 0. # loss must be tracked; otherwise flux cannot use it
    nearestNeighbourIDs = zeros(Integer, batchSize)

    for i in 1:batchSize
        kLargestIDs = selectperm(similarity[:, i], 1:memory.k, rev = true)
        nearestNeighbourIDs[i] = kLargestIDs[1];
        loss += memoryLoss(memory, q[:, i], findNearestPositiveAndNegative(memory, kLargestIDs, v[i]))
    end

    # Memory update - cannot be done above because we have to compute all losses before changing the memory
    for i in 1:batchSize
        memoryUpdate!(memory, normalizedQuery[:, i], v[i], nearestNeighbourIDs[i])
    end
    increaseMemoryAge(memory)

    return loss / batchSize
end

function augmentModelWithMemory(model, memorySize, keySize, k, labelCount, α = 0.1, T = Float32)
    memory = KNNmemory{T}(memorySize, keySize, k, labelCount, α)
    trainQ!(data, labels) = trainQuery!(memory, model(data), labels)
    trainQOnLatent!(latentData, labels) = trainQuery!(memory, latentData, labels)
    testQ(data) = query(memory, model(data))
    return trainQ!, testQ, trainQOnLatent!
end

end
