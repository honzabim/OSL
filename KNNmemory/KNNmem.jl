module KNNmem
#= This module implements a memory according to https://arxiv.org/abs/1703.03129
    that can be used as the last layer in a NN in the Flux framework.
=#

using Flux

export KNNmemory, query, trainQuery!

mutable struct KNNmemory
    M::Array{Float64, 2} # keys in the memory
    V::Array{Int64, 1} # values in the memory
    A::Array{Int64, 1} # age of a given key-value pair
    k::Integer # number of neighbors used in kNN
    α::Real # parameter setting the required distance between the nearest positive and negative sample in the kNN

    function KNNmemory(memorySize::Integer, keySize::Integer, k::Integer, labelCount::Integer, α::Float64 = 0.1)
        M = rand(Float64, memorySize, keySize)
        V = rand(1:labelCount, memorySize)
        A = zeros(Int64, memorySize)

        for i = 1:memorySize
            M[i,:] = normalize(M[i,:])
        end

        new(M, V, A, k > memorySize ? memorySize : k, α)
    end
end

# Partitions the list l into chunks of the length n
partition(list, n) = [list[i:min(i + n - 1,length(list))] for i in 1:n:length(list)]


# This particular version of the query is not the most efficient, however, since it is only used for evaluation and
# not training it seemed reasonable (computes the similarity n times for n queries instead of one big matrix multiplication)
query(memory::KNNmemory, q::AbstractArray{Float64, 2}) = map(x -> query(memory, x), partition(q, size(q, 1)))

# query just returns the nearest neighbour's value for a given key q
function query(memory::KNNmemory, q::AbstractArray{Float64, 1})
    similarity = memory.M * q
    largestID = indmax(similarity)
    return memory.V[largestID]
end

# For given set of k nearest neighbours, find the closest that has the same label and a different label
function findNearestPositiveAndNegative(memory::KNNmemory, kLargestIDs::Array{Int64, 1}, v::Integer)
    nearestPositiveID = nothing
    nearestNegativeID = nothing

    # typically this should not result into too many iterations
    for i in 1:memory.k
        if nearestPositiveID == nothing && memory.V[kLargestIDs[i]] == v
            nearestPositiveID = kLargestIDs[i]
        end
        if nearestNegativeID == nothing && memory.V[kLargestIDs[i]] != v
            nearestNegativeID = kLargestIDs[i]
        end
        if nearestPositiveID != nothing && nearestNegativeID != nothing
            break
        end
    end

    #= We assume that there exists such i that memory.V[i] == v
        and also such j that memory.V[j] != v

        We also assume that this won't happen very often, otherwise,
        we would need to randomize this selection (possible TODO) =#
    if nearestPositiveID == nothing
        nearestPositiveID = indmax(memory.V .== v)
    end
    if nearestNegativeID == nothing
        nearestNegativeID = indmax(memory.V .!= v)
    end

    return nearestPositiveID, nearestNegativeID
end

# Loss generated by the memory based on the lookup of a key-value pair - exactly as in the paper
memoryLoss(memory::KNNmemory, q::AbstractArray{Float64, 1}, nearestPosAndNegIDs::Tuple) = memoryLoss(memory, q, nearestPosAndNegIDs...)

function memoryLoss(memory::KNNmemory, q::AbstractArray{Float64, 1}, nearestPositiveID::Integer, nearestNegativeID::Integer)
    normalizedQ = normalize(Flux.Tracker.data(q))
    loss = max(dot(normalizedQ, memory.M[nearestNegativeID, :]) - dot(normalizedQ, memory.M[nearestPositiveID, :]) + memory.α, 0)
end

# function that computes the appropriate update of the memory after a key-value pair was lookedup in it
function memoryUpdate!(memory::KNNmemory, q::AbstractArray{Float64, 1}, v::Integer, nearestNeighbourID::Integer)
    normalizedQ = normalize(Flux.Tracker.data(q))

    # If the memory return the correct value for the given key, update the centroid
    if memory.V[nearestNeighbourID] == v
        memory.M[nearestNeighbourID, :] = normalize(normalizedQ + memory.M[nearestNeighbourID, :])
        memory.A[nearestNeighbourID] = 0

    # If the memory did not return the correct value for the given key, store the key-value pair instead of the oldest element
    else
        oldestElementID = indmax(memory.A + rand(1:5))
        memory.M[oldestElementID, :] = normalizedQ
        memory.V[oldestElementID] = v
        memory.A[oldestElementID] = 0
    end
end

# Update the age of all items
function increaseMemoryAge(memory::KNNmemory)
    memory.A = memory.A + 1;
end

# Query that results in a change of the memory and a loss value
function trainQuery!(memory::KNNmemory, q::AbstractArray{Float64, 1}, v::Integer)
    # Find k nearest neighbours
    similarity = memory.M * q
    kLargestIDs = selectperm(similarity, 1:memory.k, rev = true)
    n1 = kLargestIDs[1]
    nearestNeighbour = memory.V[n1]

    loss = memoryLoss(memory, q, findNearestPositiveAndNegative(memory, kLargestIDs, v))
    memoryUpdate!(memory, q, v, n1)
    increaseMemoryAge(memory)

    return loss
end

# Batch version of the trainQuery
function trainQuery!(memory::KNNmemory, q::AbstractArray{Float64, 2}, v::Array{Int64, 1})
    # Find k nearest neighbours and compute losses
    batchSize = size(q, 2)
    similarity = memory.M * q # computes all similarities of all qs and all keys in the memory at once
    loss::Flux.Tracker.TrackedReal{Float64} = 0. # loss must be tracked; otherwise flux cannot use it
    nearestNeighbourIDs = zeros(Integer, batchSize)

    for i in 1:batchSize
        kLargestIDs = selectperm(similarity[:, i], 1:memory.k, rev = true)
        nearestNeighbourIDs[i] = kLargestIDs[1];
        loss = loss + memoryLoss(memory, q[:, i], findNearestPositiveAndNegative(memory, kLargestIDs, v[i]))
    end

    # Memory update - cannot be done above because we have to compute all losses before changing the memory
    for i in 1:batchSize
        memoryUpdate!(memory, q[:, i], v[i], nearestNeighbourIDs[i])
    end
    increaseMemoryAge(memory)

    return loss / batchSize
end
end
